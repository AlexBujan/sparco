\chapter{The statistical structure of laminar recordings}
\label{chap:sparse}

\section{Introduction}

% Neuroscience has experienced a period of remarkable advancement in
% recording technology in recent years. A variety of methods are now
% available to record from populations of neurons simultaneously,
% including two-photon and multi-photon Ca$^{2+}$
% imaging~\cite{Bock:2011uq,Mittmann:2011uq}, implantable chronic arrays
% of electrodes~\cite{Santhanam:2006kx}, fiber optic fluorescence
% microendoscopy~\cite{Flusberg:2005vn}, and silicon
% polytrodes~\cite{Blanche:2005uq}. The data produced from these
% recording methods pose a number of new challenges, requiring
% development of novel analysis techniques. The quantity of data
% recorded is often orders of magnitudes larger than that produced by
% more traditional methods, requiring among other things that analysis
% methods be computationally efficient and scalable. Additionally, the
% data is high dimensional in nature and needs to be transformed into
% reduced representations or split up into tractable subcomponents in
% order to characterize in exploratory analysis. The statistics of the
% data may be non-stationary, inconsistent across repeated experiments
% and containing experimental artifacts that are difficult to account
% for and remove. Often the properties of the recording device itself
% are poorly understood, or it is unclear ahead of time what the data
% should look like with generic analysis methods producing misleading
% results.

% [TODO] 
% - Statement of problem
% - Sparse coding
%  - existing methods are insufficient
%  - learn higher order statistics, beyond gaussian, PCA
%  - higher order structure

% More comments on Chapter 2:

% The first paragraph is throw-away, too general, it is background that
% belongs in Chapter 1.  You need to begin this chapter by referring
% back to the recordings you showed in Chapter 1, the rich forms of
% structure.  You need to begin motivating what you are going to do in
% this chapter.  How to characterize this structure?  You can't just
% start off saying "I'm going to do sparse coding."  You need to discuss
% some of the alternatives, standard ways people currently approach this
% and their shortcomings, and how you intend to overcome these
% shortcomings with your approach.  Then you can say something about
% sparse coding, what it is, and why it is a sensible approach.  That
% should be your intro to Chapter 2.

% Here is what I recommend for the layout of Chapter 2:

% Introduction

% Laminar recordings - here you say a bit about how the data were
% collected, the stimuli used, the histology slide which is very nice.
% Basically strip down what you have now in 'Methods'

% Sparse coding model
% - describe the model and your implementation, basically as you already have it in Methods

% Results
% - first describe the results on test data, which you currently have at the end.  The point is that before applying the algorithm to neural recordings you want to verify that it works and it does what you expect it to do, that will give the reader some confidence your results on neural data are sensible.
% - Then describe the results on spike data
% - then describe the results on LFP
% - then describe your efforts to use the algorithm for spike sorting and denoising

% Discussion
% - you should say more here - what are the shortcomings of this approach?  why do you think its worth pursuing?  reiterate its novelty in comparison to other methods, i.e. standard spike sorting, filtering, etc.

Silicon polytrodes have become increasingly commonplace in recordings
from a variety of brain areas. Built out of a silicon substrate with a
high-density of recording sites, they are available in a variety of
site arrangements and physical
configurations~(Fig.~\ref{fig:polytrodes}). They enable recording the
extracellular waveforms of populations of neurons at a high spatial
and temporal resolution. However, they share many of the challenges of
other large-scale recording methods. Traditional spike sorting
algorithms, designed for single electrodes or tetrodes, are in general
not scalable to the increased number of channels and often cannot
incorporate and exploit knowledge of the physical geometry of the
probe. Polytrodes may have poorer single unit isolation than single
electrodes as their position cannot be finely adjusted to move sites
near cell bodies. Super-imposed spike waveforms on multiple channels
cannot be handled by conventional thresholding and clustering spike
sorting methods. Traditional signal processing algorithms such as
spectral density estimation and linear filtering methods treat
channels independently. It is not clear how to incorporate the complex
phase relationships between polytrode channels in fourier-based
analysis methods such as spike-field
coherence~\cite{Bokil:2010ly}. These methods make implicit assumptions
about the nature of recordings that may not hold, such as ascribing
special significance to activity in narrow frequency bands. The
statistics of the recorded data may be non-stationary, inconsistent
across repeated experiments and containing experimental artifacts that
are difficult to account for and remove. 

% These issues make analysis of polytrode data a daunting task. What is
% needed is a method that can handle the data in its full complexity,
% that is robust to experimental confounds, that is computationally
% efficient enough to process hours of recordings, and that is not full
% of arbitrary assumptions about the structure of the data. In contrast
% to traditional signal processing methods, latent variable models
% (Sec.~\ref{sec:sparseintro}) are statistical models that attempt to
% explain the structure of data through unobserved parameters. For
% example, the signal recorded on an electrode reflects biophysical
% processes occurring in the vicinity of a contact, such as synpatic
% currents in dendrites or action potentials travelling along
% axons. These are the underlying `causes' of the observed signal. A
% latent variable model, given some prior knowledge of the system, can
% infer these causes. Additionally, such models can learn an appropriate
% representation of the data in an unsupervised fashion such that the
% inferred causes have direct relationship to physical phenomena. This,
% however, does come at a cost as inference and learning can be
% computationally intensive with algorithms that require sophistication
% and that may not always give desired results. However, given the
% dramatic increase in available computational power and advances in
% methods for numerical optimization, the explanatory power of latent
% variable models is an attractive alternative to traditional signal
% processing methods.

In this Chapter, I present a novel application of sparse
coding~\cite{Olshausen96} (Sec.~\ref{sec:sparseintro}) to polytrode
recordings from visual cortex. In this case, the underlying data is
modeled as an approximately linear superposition of electric
potentials from nearby biophysical events.  Sparse coding assumes
these events occur sparsely in time and in space along the
polytrode. It is able to learn a natural representation for these
events such the causal structure of the data is explicit.
% In the case of high-pass
% filtered polytrode data, we show the representation learned are a set
% of spike waveforms, with infered causes having some correspondence to
% spike times. In the case of LFP, the results are more difficult to
% interpret, but still provide a new approach with few assumptions about
% the data in a representation learned from the statistics of the signal
% itself.
The polytrode devices in this case are single shaft probes with 32 or
54 channels that span the extent of a cortical column. The goal is to
record from a population of 30-40 neurons across all cortical lamina
to study the spiking and LFP response properties of the population in
response to dynamic, natural stimuli. Sparse coding models were first
applied to natural image and audio
statistics~\cite{Olshausen96,Smith:2006qf,karklin2008emergence} and
have recently been used in a wider range of applications. The simplest
version of a sparse coding model, which is used here, is a linear
generative model with a sparse prior on the latent variables. The
basis functions learned provide a decomposition of the signal into
dominant statistically recurring components. Their characteristics are
revealing and interpretable and have the potential to shed light on
structure that would be difficult to discern through other methods
such as principal components analysis (PCA). An application is
demonstrated in Chap.~\ref{chap:responses}. Additionally, the inferred
latent variables can be used as regressors in subsequent analyses in
the place of the original data (Chap.~\ref{chap:glm}).

The data was divide into high and low frequency, or `spike' and `LFP'
datasets, respectively, and a sparse coding basis was learned per
recording penetration. The learned spike bases represent components of
extracellular potentials of individual neurons along the length of
polytrode. Their relationship to spike sorted data using more
conventional methods is demonstrated and a number of applications are
presented. The learned LFP components reveal a rich laminar structure
that is separated into distinct frequency bands. This novel
representation of the LFP provides a promising new approach to
understanding the biophysical cause and significance of these signals.

\section{Methods}

\subsection{Surgery and preparation}
\label{sec:surgery}

Acute experiments were performed on three female (2.8-3.5~kg) and one
male (3.5~kg) anesthetized and paralyzed cats. Prior to surgery, the
animals were anesthetized with an intramuscular injection of ketamine
(12~mg/kg) and acepromazine (0.3~mg/kg) and given atropine
(0.025~mg/kg) subcutaneously to reduce salivation.  Anesthesia was
maintained using halothane (0.6-2\%) in a mixture of nitrous oxide and
oxygen (2:1) using a Surgivet Model 100 halothane vaporizer while the
animals were actively ventilated using a Harvard Apparatus respirator
pump. The cephalic vein was cannulated and a continuous infusion of
Ringer's solution containing 2.5\% dextrose was given throughout the
experiment at a rate of 4~ml/kg/h.
% sanity check: IV set 15drops/ml. this is 12ml/h = 180d/h = 3dpm or
% one 1000ml bag in 83h. We used 1.5l (pretty sure) over 72 at more
% like 5dpm, which is roughly in the ballpark.
The antibiotic Cephazolin was administered intramuscularly (30~mg/kg)
every eight hours to reduce the risk of infections. Vital signs
including electrocardiogram, heart rate (140-180~bps), rectal body
temperature ($37-39^\circ$ C), end-tidal CO$_2$ (3.5-4.5\%), and
SpO$_2$ (99-100\%), were continuously monitored and maintained within
the normal range using a Cardell 9500 HD veterinary life signs monitor
and a Gaymar TP650 thermal water blanket.

The animals were mounted in a stereotaxic frame and a stainless steel
head post was affixed anterior to the craniotomy location using 6-8
stainless steel screws and covered with dental acrylic. This allowed
us to retract the ear and eye bars to remove potentially painful
pressure points and also served to electrically isolate the animal
from the stereotaxic table. The stereotaxic frame was used to locate
the primary visual cortex at coordinates 67~mm dorsally and $\pm 3$~mm
% cite atlas
laterally relative to AP0. After removal of the
eye and ear bars, the head remained suspended from the head post for the
remainder of the experiment.

An approximately 7~mm $\times$ 7~mm craniotomy was made in one
hemisphere over area 17. After recording from several penetrations in
this hemisphere, a second craniotomy was made in the opposite
hemisphere for further penetrations. Paralysis was then induced with
vercuronium at 1~mg/kg and maintained at 1~mg/kg/h intravenously.

A local application of Neosynephrine was made to retract nictitating membranes
as well as an application of atropine to dilate the pupils. The eyes were
focused on a computer monitor screen at a distance of 57~cm using a pair of
gas permeable contact lenses chosen using the tapetal reflection technique
\cite{Pettigrew:1979fk}, where an opthalmoscope light introduced into the eye
produced an in-focus, reflected fundus image on a white screen facing the
animal. At this distance, $1^\circ$ of visual angle corresponds to 1~cm on the
computer monitor.

To reduce pulsations from heartbeat and relieve intracranial pressure during
the experiment, a catheter was inserted into the cisterna magna to allow for
the partial drainage of cerebrospinal fluid. A small portion of the dura mater
and, as necessary, the arachnoid layer, were then reflected using a dural
knife custom made from a surgical needle. The head stage holding a silicon
polytrode was carefully positioned just above the cortical surface using Kopf
Instruments micromanipulators. A 4\% mixture of agar in artificial
cerebrospinal fluid (aCSF) was applied to protect the cortical surface and
reduce pulsations. The polytrode was slowly lowered perpendicularly into
cortex using a hydraulic microdrive Narishige Model MHO 110. The
craniotomy and a portion of the polytrode assembly where then covered with a
layer of bone wax to prevent the agar from drying out.

The protocol used in these experiments was approved by the Institutional
Animal Care and Use Committee at Montana State University.

\subsection{Recording procedure}

All recordings were performed using single shank, planar silicon
polytrodes purchased from NeuroNexus Technologies. In the first
experiment, a 54-channel probe was used with contacts staggered in two
columns, spaced 65~$\mu$m vertically, 56~$\mu$m horizontally, with an
overall shaft width of 206~$\mu$m ($54\mu$map2b in
\citet{Blanche:2005uq}). In the remaining three experiments,
32-channel probes were used with sites arranged in a single column
with contact diameters of 15~$\mu$m, 23~$\mu$m, and 30~$\mu$m arrayed
over a length of 1.55~mm with full-shaft lengths of 6~mm or 10~mm and
width tapering from 250~$\mu$m to a point over the active length of
the probe (Fig.~\ref{fig:polytrodes}a).
% [U] minimum width not meaningful since it pretty much tapers to a
% point. Thickness 15mu, width of shank about 250 but there is no hard
% number for this, I used a ruler on the drawing in the catalog.

% [U] are you going to mention the multi-tetrodes as well? 
% [A] Not sure, I put the rasters in the introduction. I should maybe
% add a pointer to Yen, Baker, Gray (2007)

The polytrode was mounted onto a Multichannel Systems ADPT-NN-32 Probe
adapter, which was connected to the hydraulic drive by a custom
adapter.  Signals were buffered with a head-stage amplifier with gain
of 10 in the first three experiments, modified to a gain of 5 in the
last, using Multichannel Systems model MPA32I. The signals were
further amplified with a Multichannel Systems model FA64I amplifier
with a gain of 500 and cutoffs at 1~Hz and 10~kHz.  This signal was
digitized with 14-bit precision and 30~kHz sampling rate with a
PC-based data acquisition system containing a United Electronic
Industries PowerDAQ PD2-MF-64-2M-14H. The broadband signals for all
channels were recorded to hard disk for offline processing. For the
last experiment, electrolytic lesions were made at the end of each
recording session by injecting 2~$\mu$A of current for a duration of
12~s into the top and bottom recording sites according to the probe
manufacturer's specification. However, lesions were not visible in the
histology and did not aid in recovering polytrode position.

% [A] Impedance testing?

The raw data was divided into low-pass and high-pass datasets. For the
high-pass dataset, a 4-th order Butterworth filter with passband
between 0.5~kHz and 10~kHz was applied forward then backward on the
full raw dataset for a net zero-phase filtering and the filtered data
saved to disk without resampling at 30~kHz. For the low-pass dataset,
the data was down-sampled from 30~kHz to 1~kHz by a succession of
filtering and sub-sampling steps of 5, 3, and 2 times, with FIR
filters designed using the window method and filter lengths of 256,
128, and 128, respectively. This data was then filtered with a
zero-phase Butterworth filter with passbands of 1 to 150~Hz and saved
to disk at 1~kHz sample rate. These two datasets are referred to as
the spike and local field potential (LFP) datasets. All analysis
software was custom written using a collection of open-source
\texttt{python} tools, including \texttt{numpy},
\texttt{scipy}~\cite{numpyscipy}, \texttt{ipython}~\cite{ipython},
\texttt{h5py}~\cite{h5py}, and \texttt{matplotlib}~\cite{matplotlib}.

\subsection{Histological procedures}

At the end of each experiment, the animal was euthanized with an
intravenous injection of pentobarbital and perfused with a 4\%
formaldehyde solution in phosphate buffered saline. The brain was then
removed and brain regions containing the recording locations were
blocked and fixed in sucrose solution. Histological Nissl-stained
slices of 60~$\mu$m thickness were made (FD~Neurotechnologies) in a
coronal direction at 120~$\mu$m intervals. For all but one
penetration, the polytrode track was clearly visible in the slices. An
example is shown in Fig.~\ref{fig:histology}. The tracks were used to
confirm recording depth, angle of penetration, and whether the
recording spanned an entire single or multiple cortical columns.

\begin{figure}[ht!] 
  \centering
  \includegraphics[width=1\textwidth,page=2,trim=0 1.5in 2in 0]{Ch2_fig/key2.pdf} 
  \caption{\textbf{Histological recording signature.} \textbf{(a)}
    Full brain from one experiment showing recording site and location
    of visual areas 17 and 18 as well as a coronal block in
    preparation for histology. \textbf{(b)} Reconstruction of one
    penetration from four superimposed Nissl-stained slices. In each
    of the four slices, some tissue damage due to the electrode
    passage is visible. The penetration spans all cortical layers and
    is aligned parallel to the striations of cortical mini-columns. Due
    to tissue shrinkage from the perfusion and fixation process, as
    well as the penetration spanning multiple slices,
    % (which indicates
    % that the block was not cut exactly parallel to the penetration)
    the scale bar does not exactly correspond to the span of recording
    locations of the polytrode.}
  \label{fig:histology} 
\end{figure}
\clearpage

\subsection{Stimuli}

A variety of visual stimuli were used in each experiment, including
long and repeated short natural movies, `Hilbert' movies, drifting
gratings, binary white noise, and full-field black and white
stimuli. The combined duration of all movies played for each
penetration was approximately four hours. The stimuli and presentation
protocol are presented in detail in Sec.~\ref{subsec:stimuli}.

\subsection{Sparse coding model}
\label{sec:sparsealg}

Sparse coding~\cite{Olshausen96,olshausen2003learning,Smith:2006qf} is
a latent variable model that attempts to describe data in terms of a
small number of additive components, or basis functions, selected out
of a large dictionary. Let $y_i(t)$, the data on channel $i$ at time
$t$, be written as a temporal convolution of a set of basis functions
$\phi_{ij}(t)$, with $i$ and $j$ denoting channel and basis function,
respectively,
\begin{align}
y_i(t) = \sum_j \phi_{ij}(t) * x_j(t) + \epsilon_{i}(t) \label{eqn:conv}
\end{align}
with $\epsilon_i(t) \sim \mathcal{N}(0,\sigma_n)$ small, uncorrelated
gaussian noise on each channel. This model is illustrated in
Fig.~\ref{fig:sparseconvolution}. To estimate model parameters, the
data is assumed to be an identically distributed, independent ensemble
of length $T$ time samples with $C$ channels $\mb{Y} =
\{\mb{y}^{(i)}\}_{i=1\ldots D}$ with $\mb{y}^{(i)} \in \mathbb{R}^{C
  \times T}$. The log-likelihood of the model is,
\begin{align*}
\mathcal{L}(\mb{\Phi}, \sigma_n)
&= \log p(\mb{Y} | \mb{\Phi}, \sigma_n, \lambda) \\
&= \sum_{i=1}^D \log p(\mb{y}^{(i)} | \mb{\Phi}, \sigma_n, \lambda) \\
&= \sum_{i=1}^D \log \int \mathrm{d} \mb{x} \, p(\mb{y}^{(i)}
| \mb{x}, \mb{\Phi}, \sigma_n) \, p(\mb{x} | \lambda)
\end{align*}
where,
\begin{align*}
  p(\mb{y} | \mb{x}, \mb{\Phi}, \sigma_n) \propto
  \exp\left(-\frac{1}{2 \sigma_n^2} \sum_{t=1}^T \norm{\mb{y}_t -
      \sum_\tau \mb{\Phi}_\tau \mb{x}_{t-\tau}}^2\right)
\end{align*}
with $\mb{\Phi}_\tau \in \mathbb{R}^{C \times N}$, $\mb{x}_\tau \in
\mathbb{R}^N$ with $N$ the number of basis functions. The sparse prior
on the coefficients $\mb{x}$ is parametrized with $\lambda$. The goal
of learning is to maximize the likelihood $\mathcal{L}(\mb{\Phi},
\sigma_n)$. The derivative of $\mathcal{L}(\mb{\Phi}, \sigma_n)$ is
taken with respect to the model parameter $\mathbf{\Phi}_\alpha$,
\begin{align*}
  \frac{\partial \mathcal{L}}{\partial \mathbf{\Phi}_\alpha} &\propto
  - \sum_{i=1}^D {\int \mathrm{d} \mb{x} \, p(\mb{x} | \mb{y}^{(i)},
    \mb{\Phi}, \sigma_n, \lambda) \sum_{t=1}^T (\mb{y}_t - \sum_\tau
    \mb{\Phi}_\tau \mb{x}_{t-\tau}) \,\mb{x}_{t-\alpha}^T}
\end{align*}
A number of ways exist to estimate the intractable integral in this
expression, including a Laplace
approximation~\cite{lewicki1999probabilistic} and Hamiltonian Monte
Carlo sampling~\cite{culpepperbuilding}. The simplest approach, taken
here, is to assume the posterior distribution $p(\mb{x} |
\mb{y}^{(i)}, \mb{\Phi}, \sigma_n, \lambda)$ is sufficiently peaked
and to take one sample at its mode~\cite{Olshausen97}, that is,
\begin{align}
  \mb{x}^{(i)} &= \argmax_{\mathbf{x}} p(\mb{x} | \mb{y}^{(i)},
  \mb{\Phi}, \sigma_n,
  \lambda) \nonumber \\
  &= \argmax_{\mb{x}} \log p(\mb{y}^{(i)}, \mb{x} |\mb{\Phi},
  \sigma_n, \lambda) \nonumber \\
  &= \argmin_{\mb{x}}\left( \frac{1}{2 \sigma_n^2} \sum_{t=1}^T
    \norm{\mb{y}_t - \sum_\tau \mb{\Phi}_\tau \mb{x}_{t-\tau}}^2 -
    \log p(\mb{x}|\lambda) \right)\label{eqn:inference}
\end{align}
Model likelihood was maximized using an alternating scheme where
$\mb{x}^{(i)}$ were inferred and then used to update
$\mathcal{L}(\mb{\Phi},\sigma_n)$~\cite{Olshausen97}. An additional
simplifying assumption was made to take the parameter of the gaussian
noise $\sigma_n$ as given, though a prior could be imposed on it and
estimated along with $\mb{\Phi}$. In practice, an appropriate
$\sigma_n$ is approximated from descriptive statistics of the data. As
the data is practically infinite, only a small sample of
$\mb{y}^{(i)}$ was chosen in each step. Additionally, the model has a
degeneracy due to the sparse prior $p(\mb{x}|\lambda)$ shrinking
coefficients to zero, causing the norm of $\mb{\Phi}$ to grow without
bound. Therefore a convex constraint was imposed such that,
\begin{align*}
  \sum_{i=1}^C\sum_{\tau=1}^P\phi_{ij\tau}^2 \le 1
\end{align*}
where $P$ are the number of time taps in the basis functions. The
constraint makes the learning update of $\mb{\Phi}$ a quadratically
constrained quadratic program (QCQP)~\cite{boyd2004convex}. In
practice, however, this problem was solved by making a small
stochastic gradient learning step and renormalizing the basis
functions on each iteration. A full QCQP solver was implemented using
a convolutional adaptation of the method proposed in
\citet{mairal2010online}, but found that for neural datasets, the
algorithm was prone to get stuck in local minima. During learning, the
basis functions were recentered gradually over many iterations. This
reflected an implicit prior that the basis functions should be
temporally localized.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=0.9\textwidth,page=1,trim=0 4in 3in 0]{Ch2_fig/key2.pdf}
  \caption{\textbf{Convolutional sparse coding.} A convolutional
    sparse coding model is a linear generative model where data is
    represented as a sum of a convolution of a set of bases with
    coefficients which are expected to be sparse and independent in
    both time and channel. This schematic shows how to reconstruct a
    portion of one channel of data at bottom using two basis elements
    at left convolved with corresponding sparse coefficients at right
    and summed. Estimation of basis functions $\phi_{ij}$ and
    coefficients $x_j$ (Eq.~\ref{eqn:conv}) are called
    `learning' and `inference', respectively. Bases $\phi_{ij}$ are
    learned for spiking and LFP datasets separately for each recording
    penetration and coefficients are inferred for the entire
    recordings. This new representation has many desirable
    properties.}
  \label{fig:sparseconvolution}
\end{figure}

\subsubsection{Matching pursuit inference}

The nature of the spiking and LFP datasets required the inference
problem~\ref{eqn:inference} to be solved the using a different
strategy for each case. For the spike dataset, which is made up
primarily of highly sparse spike activity separated in time as well as
in space among the channels and mixed with approximately gaussian
noise, a greedy algorithm, matching
pursuit~\cite{mallat1993matching,olshausen2003learning,Smith:2006qf},
was chosen. This algorithm is efficient for a high degree of assumed
sparsity when the basis functions can be assumed to be relatively
incoherent. Its goal is to represent the data with at most $k$ basis
functions,
\begin{align*}
  \mb{x}^* = \argmin_{\card{\mb{x}} \le k} \frac{1}{2 \sigma_n^2}
  \sum_{t=1}^T \norm{\mb{y}_t - \sum_\tau \mb{\Phi}_\tau
    \mb{x}_{t-\tau}}^2
\end{align*}
where $\card{\mb{x}}$ denotes the number of non-zero elements of
$\mb{x}$. For arbitrary $\mb{\Phi}$, this problem is
NP-hard\cite{davis1997adaptive}. Matching pursuit is a greedy strategy
that finds an approximate solution and overcomes this combinatorial
complexity. Additionally, to make the learned basis functions and
coefficients more physiologically interpretable, the coefficients were
forced to be non-negative.

\subsubsection{L$_1$-regularized quasi-Newton inference}
\label{sec:quasinewton}

For the LFP dataset, an L$_1$-regularized method was used to induce
sparsity on the coefficients. The LFP dataset is sampled at a lower
rate and is hence considerably smaller than the spike dataset,
reducing requirements for computational efficiency by a large
factor. An L$_1$ method performed better at learning in a less sparse
regime where basis functions were more coherent and `explaining away'
was more critical. Briefly, the prior on coefficients was assumed to be
exponentially distributed,
\begin{align*}
  p(\mb{x}|\lambda) \propto e^{-\lambda \norm{\mb{x}}_1}
\end{align*}
giving the following convex minimization problem for inference,
\begin{align*}
  \mb{x}^* = \argmin_{\mb{x}>0} \frac{1}{2 \sigma_n^2} \sum_{t=1}^T
  \norm{\mb{y}_t - \sum_\tau \mb{\Phi}_\tau \mb{x}_{t-\tau}}^2_2 +
  \lambda\norm{\mb{x}}_1
\end{align*}
This objective is closely related to the Lasso, which has been
intensively studied~\cite{tibshirani1996regression,chen1999atomic} and
for which an abundance of methods exist. Despite its widespread use as
a feature selection method, it is important to note that an L$_1$
regularizer has a deficiency. If the data is indeed generated by a
Laplacian distribution, its order statistics are not sufficiently
sparse to guarantee recovery~\cite{baraniuk2010low}. Additionally,
both inference methods used are not causal, and coefficients inferred
for a given time can be affected by data in the future. In contrast,
state-space models such as Kalman filters and hidden Markov models are
causal by design, though operations such as smoothing are inherently
acausal. Creating a causal inference algorithm in this setting is an
open problem that is the subject of future work.

\subsection{Implementation}

For convolutional matching pursuit, the algorithm was implemented in
\texttt{python} using the \texttt{numpy}~\cite{numpyscipy}
library. For L$_1$-regularized inference, a
method~\cite{andrew2007scalable} based on the widely used l-BFGS
quasi-Newton algorithm~\cite{Liu89} was chosen, which uses a
particular choice of sub-gradient whenever the optimization attempts
to cross from one octant to another. The advantages of this algorithm
over the many others is that it does not require computing the Gram
matrix, only requires an objective and gradient to be defined, is
numerically stable for large numbers of parameters, converges quickly
to an approximate solution, and can be efficiently run in parallel on
multiple cores of a processor as it uses only BLAS level 1 operations.

Three versions of this algorithm were implemented. The first was a
\texttt{cython}\cite{behnel2011cython,seljebotn2009fast} wrapper of
the \verb!C++! library \texttt{liblbfgs}~\cite{liblbfgs}, which
explicitly implements all linear algebra with SSE2 instructions. Next,
a version in \texttt{cython} was implemented that could handle a
vector of L$_1$ regularization parameters $\lambda$, a non-negative
constraint on the coefficients, and would run more efficiently on
multicore architectures through its use of vendor linear algebra
libraries. Lastly, a version in \texttt{cython} and
\texttt{pycuda}~\cite{pycuda} was implemented to run fully on the
NVIDA GPU avoiding all host to GPU transfers during the
optimization. In this implementation, the convolutions in the
objective were implemented both as a bank of 2-D FFTs as well as a
single 3-D FFT. The 3-D FFT method, despite its theoretical
inefficiency in this case, gave an approximately 30x speed-up versus
computing the convolutions in the time domain on the CPU. The 2-D FFT
bank gave only a 6x speed-up, most likely due to the GPU being
constrained to computing one 2-D FFT at a time. Array slicing was
implemented with 1-D texture maps and all norms, projections, and
reductions were custom written to take advantage of the parallelism in
the GPU.

The full learning algorithm was parallelized using the Message Passing
Interface (MPI) in \texttt{python} using
\texttt{mpi4py}~\cite{seljebotn2009fast}. On each learning iteration,
the root node sampled the data from disk and distributed this data
amongst the nodes using an MPI \texttt{Scatter}. All nodes then
performed inference using one of the algorithms described above and
the results were reduced on the root node with an MPI
\texttt{Gather}. A learning step was taken and the basis was then MPI
\texttt{Broadcast} to all nodes. This framework allowed the learning
algorithm to exploit parallelism on a single multicore CPU with or
without a GPU, a cluster of multicore CPU's, as well as a hybrid
cluster of CPU's and GPU's.

\subsubsection{Parallel inference of coefficients for a full dataset}
\label{sec:parallelblock}

After a basis was learned for a spike or LFP dataset, coefficients
were inferred for the whole dataset in a chunk-wise parallel fashion
(Fig.~\ref{fig:parallelblock}). Given $N$ parallel computational
nodes, the data was divided into $N$ large chunks. Within each chunk,
inference was performed in sequence on blocks of time length $T$ with
$T >> P$, where $P$ is the number of time taps in the learned basis
$\mb{\Phi}$. Blocking was used at it is computationally impractical to
perform inference on arbitrarily large time segments. A block of
length $T$ yielded coefficients $\mb{x}$ of time length $T+P-1$. After
one block was completed, all except a $P-1$ length of its tail was
written to disk. The $2P-2$ tail portion of this block was used for
initializing the next block. For the new block, the first $P-1$
coefficients were held fixed whereas the next $P-1$ coefficients were
used to warm start the inference in the case of L$_1$ or were set to
zero for matching pursuit.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=0.9\textwidth,page=6,trim=0 1.25in 2.in 0]{Ch2_fig/key2.pdf}
  \caption{\textbf{Inferring coefficients over a full dataset.}
    \textbf{(a)} Each parallel chunk of data is blocked into tractable
    portions and processed serially by using tail portions of previous
    blocks to warm-start the following block. \textbf{(b)} When each
    parallel chunk is completed, the regions between chunks are
    processed to stitch chunks into one long set of coefficients. The
    stitched portion is warm-started from it's adjoining blocks.}
  \label{fig:parallelblock}
\end{figure}

The non-linear nature of inference raised the possibility that the
implementation would have blocking artifacts. However, inference was
tested by blocking over a several block region as well as inferring
coefficients on the whole region with results in good agreement. This
is due to the high degree of sparsity used as well as a side-effect of
the convolutional formulation of the objective, that coefficients on
the $P-1$ borders received less derivative information and were more
likely to remain at zero.

Parallel blocks were stitched by performing inference on adjoining
regions of length $3P-2$, with only the inner $P-2$ portion being free
to be modified by the optimization. The parallel blocking allowed the
inference to scale almost linearly and to compute coefficients for a
recording session with approximately the same order of time as the
experiment itself. The datasets with metadata were written to disk
using a generic gzip level 4 compressed HDF5 format which afforded a
20-100 fold compression over the original dataset, depending primarily
on the level of coefficient sparsity.

\subsection{Conventional spike sorting}

The dataset from one penetration was spike sorted using conventional
methods for evaluation of the spike coefficients inferred using sparse
coding. The high-pass dataset was divided into 8 groups of 4
non-overlapping channels, serving as virtual tetrodes. Putative spike
waveforms were extracted using a threshold crossing criterion of 5
standard deviations after removing data outliers. A custom set of 11
features were computed for each extracted spike. The features were
input into KlustaKwik~\cite{Harris:2000fk}, which uses a modified
mixture-of-gaussians clustering algorithm, and the result was loaded
into MClust~\cite{mclust}. Clusters were cleaned up by manually
deleting and merging waveforms as necessary and saved as single
units. Special care was given to identifying bursting cells, which
have waveforms that change in amplitude and shape with each subsequent
firing.  After sorting for all virtual tetrodes were completed, cells
with waveforms in adjacent tetrodes were identified and merged into
single cells.

\subsection{Model data}

To create realistic model spike data~\cite{einevoll:2011sp} for
testing the learning of spike waveforms and comparing model spike
times with inferred sparse coefficients, the modeling software
LFPy~\cite{linden2011} was used. LFPy allows the specification of
compartmental models of a population of neurons with given morphology,
simulates them using Neuron~\cite{hines1997neuron}, and records the
extracellular potential at a set of electrodes using a line-source
approximation~\cite{Holt:fk}. To generate data, an example morphology
provided by LFPy of a layer 5 pyramidal neuron~\cite{Mainen:1995uq}
was used, with three sets of active
conductances~\cite{Hendrickson:2011fk}. A set of 20 neurons were
placed randomly with uniform probability along the length of a
32-channel model polytrode with distance from the probe distributed
according to a gaussian distribution with a standard deviation of
30~$\mu$m. All neurons were oriented identically, roughly parallel to
the polytrode. The model was run for 10 seconds at 31.25~kHz with each
neuron's ion channels driven by constant magnitude synaptic inputs
with gamma distribution arrival times.  This data was then high-pass
filtered between 0.5-10~kHz and a spike basis was learned. Sparse
coefficients were inferred to compare with model spike times. To make
learning and inference more challenging, varying degrees of noise was
added to each channel of the filtered model data. This framework will
be used in future work to study the extracellular waveforms of
different types of model neurons, to be able to assign cell types to
recorded waveforms, to understand and identify other phenomenon such
as back-propagating action potentials, to better understand the
properties of the recording device itself, and to study the aggregate
activity of a population of neurons in a model cortical column. Soma
traces, synaptic currents, and recorded extracellular activity for one
neuron are shown in Fig.~\ref{fig:lfpyneuron}.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=1.0\textwidth,page=7,trim=0 2.75in 0.75in 0]{Ch2_fig/key2.pdf}
  \caption{\textbf{Generating model spike data.} An
    LFPy~\cite{linden2011} model layer 5 pyramidal cell with AMPA
    (red), NMDA (magenta), GABA$_\mathrm{A}$ (blue) active channels
    distributed along the its dentritic arbors. \textbf{(a)} Soma
    trace with three action potentials. \textbf{(b)} Synaptic current
    traces in all channels. Input arrival times are gamma distributed.
    \textbf{(c)} Recorded extracellular activity at the 32-channel
    contact positions with time. \textbf{(d)} Two views of the
    geometry of the neuron, distribution of channels, and probe
    contacts (green).}
  \label{fig:lfpyneuron}
\end{figure}

To create model data for testing the learning of LFP basis functions,
a set of spatiotemporal Gabor functions were distributed across
channels, with coefficients sampled from Bernoulli or Laplace
distributions, and reconstructed raw data from them. Varying degrees
of noise were added including gaussian noise, sinusoidal line noise,
and sinusoidal line noise mapped through a nonlinearity. The resulting
model data shared many of the properties of the real LFP, including
activity across lamina with both coherent and non-trivial phase
relationships.

\section{Results}

\subsection{Learned bases}

\subsubsection{Spike basis functions}
\label{sec:spikebasis}

The linear generative convolutional sparse coding model
(\ref{sec:sparsealg}) was applied to a spike dataset consisting of
high-pass filtered data from a single, 4 hour penetration. A linear
generative model is particularly well-suited to this problem as the
biophysical system in which the polytrode is embedded can be
approximated as an isotropic, resistive medium satisfying the
quasi-static approximation of Maxwell's equations. The signal recorded
at the electrode contacts can be interpreted as linearly added
potentials of extracellular activity surrounding the
contacts~\cite{Pettersen:2008tn}.

The learned basis, sorted by lamina, is shown in
Fig.~\ref{fig:spikebasis}. The basis elements divide into a number of
distinct classes. Many basis elements are localized in time and
channels, with shapes resembling spike waveforms. The relationship of
these elements to spiking of individual neurons is discussed
in~\ref{sec:spikesort}. A number of basis functions with coherent
activity across channels are harmonics of 60~Hz line noise bleeding
into the high-pass filtered signal. Other basis functions represent
unresolved multi-unit activity as well as activity entrained in
noise. These latter basis functions appear at much lower voltage
amplitudes in the data than the spiking activity. The basis was
learned from sample patches of data from the entire recording
penetration. A criterion that at least one channel exceed a standard
deviation threshold was used to discard patches with no spiking
activity to keep the model from trying to explain the structure in the
noise. However, since one goal of this approach was to learn all the
interesting structure in the data beyond well understood features such
as spiking activity, the sparsity enforced was lower, reconstruction
error smaller, and over-completeness of basis larger than one would use
if the goal was only to only learn spike waveforms of the best
isolated neurons. This approach introduces some confounds in
interpreting coefficients as spike times and is addressed using model
data in Sec.~\ref{sec:modelspike}. Coupled with biophysically
realistic modeling studies, this approach will allow identification of
other interesting phenomena in this frequency domain such as
back-propagating action potentials~\cite{Buzsaki:1998fk}, action
potentials along LGN axonal afferents, as well as allow assigning of
putative cell types to different classes of waveforms.

Inference was performed with matching pursuit on blocks of data that
were 4 times longer than the number of time taps in the kernels and at
most 0.5\% of coefficients were active for a given block. The number
of basis functions was chosen to be at least twice the number of
estimated single units present from visual inspection of the data so
that the model would have enough representational power to capture
low-amplitude and low-frequency spike waveforms. As a result, some
basis functions did not learn any structure and were not activated in
inference. A number of checks were performed to ensure the basis
actually represented real structure in the data. This included
re-running learning with different initial conditions, on subsets of
the recording, as well as randomly perturbing the basis during
learning, in each case checking to see to what degree the bases
changed. In general, the spike-like basis functions were highly
consistent in each case with small variations in the shapes of the
waveforms. There was a greater degree of variability in the remaining
basis functions, but, overall, the qualitative differences were
small. These variations are potentially useful for detecting
non-stationarity in the data, such as would be caused, for example, by
movement of the electrode.

\begin{figure}[htp!]
  \centering
  \includegraphics[width=\textwidth,page=3,trim=0 0 2.5in 0]{Ch2_fig/key2.pdf}
  \caption{\textbf{Learned spike basis.} \textbf{(a)} A convolutional
    sparse coding basis learned from high-pass filtered data from one
    recording penetration. Each subplot consists of a single basis
    function with polytrode channels on the vertical axis oriented
    with superficial layers at top and 2~ms of time on the horizontal
    axis. The basis functions are plotted as correlation
    kernels. Plots are in normalized units, with reds indicating
    positive voltages and blues negative voltages. Several classes of
    basis functions are readily discernible, including spike-like
    waveforms, noise basis functions correlated across channels, as
    well as unresolved multi-unit activity.  \textbf{(b)} A subset of
    basis functions in \textbf{(a)} that are localized in time and
    across channels redrawn as line plots for visualization, ordered
    from superficial to deep. These basis elements correlated well
    with spike events in the raw data.}
  \label{fig:spikebasis}
\end{figure}
\afterpage{\clearpage}

The learned basis was then used to infer coefficients for the entire
recording session. A example of inference is shown in
Fig.~\ref{fig:spikerecon}. Many neurons, particularly in the
superficial layers, fired in bursts, with waveforms that changed with
each subsequent action potential. This is captured in the sparse
coefficients as a series of activations of a single basis function in
quick succession with some change in the coefficient amplitude. Often,
many nearby neurons were active at the same time, with waveforms
overlapping across channels and time. The inferred coefficients are
able to explain away this confound to represent the data as a sum of
activations of several basis functions with overlapping
waveforms. Traditional cluster-based spike sorting
methods~\cite{Lewicki:1998ve} lack this flexibility and consequently
have difficulty separating such coincident activity. Though much of
the structure of the data is preserved in the
reconstruction~(Fig~\ref{fig:spikerecon}b), the sparsity of the
representation helped in removing some of the background noise. It is
important to remember that in contrast to traditional spike sorting
algorithms, the sparse coding model attempts to represent all of the
data and not just of waveforms extracted according to threshold
criteria.

\begin{figure}[htp!]
  \centering
  \includegraphics[width=\textwidth,page=8,trim=0 0.5in 3in 0]{Ch2_fig/key2.pdf}
  \caption{\textbf{Spike data and reconstruction.} \textbf{(a)} Sample
    high-pass filtered data, \textbf{(b)} reconstruction with basis
    shown in Fig.~\ref{fig:spikebasis}, and \textbf{(c)} sparse
    coefficients with log-magnitudes represented by size of
    ellipsoid. As the basis functions are sorted by lamina, the
    position of the coefficient activations correspond well to the
    spatial arrangement of structure in the real data. Bursting
    neurons are visible as a quick series of activations of the same
    coefficient. Neurons in close proximity tend to fire together,
    generating overlapping waveforms. The sparse coefficients are able
    to separate out these causes. Despite only .5\% of the
    coefficients being active in this case, much of the structure of
    the data is represented. }
  \label{fig:spikerecon}
\end{figure}
\afterpage{\clearpage}

\subsubsection{LFP basis functions}

The sparse coding model was also applied to the LFP. An example of a
learned basis for the same penetration as described in
Sec.~\ref{sec:spikebasis} is shown in Fig.~\ref{fig:lfpbasis}. The
basis functions, spanning 128~ms, separate into distinct classes. Many
of the basis functions are localized in time with spatial structure
that separates across lamina. Others are localized in time but are
largely coherent in space and account for relatively more of the
variance of the data than the other classes. The remaining basis
functions have low-frequency temporal structure that is partly
entrained in 60~Hz line noise. The overall sparsity of the
representation was an order of magnitude less than with the spike
bases, with approximately 10\% of the coefficients active at any time.

To learn this basis, samples were selected at random from a full
recording penetration. The sparsity penalty was initially set to a low
value to let all basis elements, which were initialized randomly, to
learn structure. The sparsity was then gradually increased from this
level during learning until the inferred coefficients had punctate
activations in time. If sparsity was too low, the basis functions,
particularly the lower frequency ones, were smeared in time making the
structure in the basis functions difficult to interpret. The 60~Hz
line noise was particularly troublesome and did not separate clearly
from low-frequency structure. This confound was addressed with modeled
LFP data with a high degree of periodic noise in
Sec.~\ref{fig:modellfp}. To ensure the learned LFP basis captured true
structure in the data, bases were learned from different portions of
the data, initialized at random in different ways, and perturbed
during the optimization to demonstrate that the learned structure was
consistent in all cases.

\begin{figure}[htp!]
  \centering
  \includegraphics[width=\textwidth,page=4,trim=0 0.3in 3.5in 0]{Ch2_fig/key2.pdf}
  \caption{\textbf{Learned LFP basis.} \textbf{(a)} A convolutional
    sparse coding basis learned from low-pass filtered data from one
    recording penetration. Basis functions are displayed as in
    Fig.~\ref{fig:spikebasis} with 128~ms on the time axis. Basis
    functions fall into a few general classes, including components
    that are localized in time with features divided across lamina,
    localized in time but correlated across lamina, with extend low
    frequency structure in time, as well as basis functions
    corresponding to 60~Hz line noise. \textbf{(b)} The same basis
    replotted as a line plot for visualization purposes.}
  \label{fig:lfpbasis}
\end{figure}
\afterpage{\clearpage}

After learning an LFP basis, inference was performed on the entire
dataset. A sample of LFP data and its reconstruction with the learned
basis is shown in Fig.~\ref{fig:lfprecon}. The LFP is relatively more
poorly understood than spiking activity. Consequently, it is difficult
to interpret the structure in the basis functions and coefficients
based on known physiology. This will be discussed in detail in
Chap.~\ref{chap:glm} where the distribution of spike and LFP
coefficients are characterized together in a joint statistical model.

\begin{figure}[htp!]
  \centering
  \includegraphics[width=\textwidth,page=9,trim=0 0.5in 2.5in 0]{Ch2_fig/key2.pdf}
  \caption{\textbf{LFP data and reconstruction.} \textbf{(a)} Sample
    low-pass filtered data, \textbf{(b)} reconstruction with basis
    shown in Fig.~\ref{fig:lfpbasis}, and \textbf{(c)} sparse
    coefficients with magnitude represented by size of ellipsoid. Only
    8\% of the coefficients were active in this case.}
  \label{fig:lfprecon}
\end{figure}
\clearpage

% Insert LFP bases learned on different parts of the data as well as
% in different penetrations

\subsection{Applications}

\subsubsection{Spike sorting}
\label{sec:spikesort}

To understand the relationship between spike bases learned through
sparse coding with actual spiking activity in the recordings, one
penetration was spike sorted using a widely used cluster-based
algorithm~\cite{Harris:2000fk}, which is referred to henceforth as
`manual' spike sorting. Spike events were detected by checking if any
channel in a segment of data had passed a threshold criterion. The
waveforms were extracted and a set of low-dimensional features were
computed from them. These features included the first few PCA
components of all waveforms, the peak-to-peak amplitude and duration
of the waveforms as well as other characteristics that reflect the
priors an experimentalist would use to categorize waveforms by visual
inspection. The features were then clustered using a gaussian mixture
model~\cite{bishop2006pattern}, where individual clusters correspond
loosely to individual neurons with means and covariances representing
the shape and variability of their waveforms in the low-dimensional
feature space. As the mixture model performs poorly with increased
dimensionality, it was necessary to block the channels on the
polytrode into a set of 8 virtual, non-overlapping tetrodes which were
sorted separately and merged in the end. The clusters were loaded into
a graphical interface and manually modified, including merging
clusters that appeared to be the same neuron based on criteria such as
auto- and cross-correlograms and inter-spike interval histograms. Some
waveforms were assigned to multi-units and others were discarded. The
process took several weeks of work with several iterations of
refinement and careful inspection by a team of people, including an
expert electrophysiologist. Overlapping waveforms and bursting were
particular troublesome and took a great deal of manual modification to
account for shortcomings of the mixture model. In contrast, learning a
spike basis and inferring coefficients for a full recording dataset
took on the order of hours when using a moderately sized parallel
cluster.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=.9\textwidth,page=5,trim=0 2.6in 7.25in 0]{Ch2_fig/key2.pdf}
  \caption{\textbf{Mean waveforms.} \textbf{(a)} Mean spike waveforms
    from 20 minutes of collected data using spike times from manually
    sorted data. \textbf{(b)} Spike basis functions learned from the
    same recording penetration for comparison. Waveforms are
    distributed across lamina, have similar temporal extent and
    structure. The spike basis functions include several waveform
    shapes not found in the manually clustered cells. Additionally,
    some waveforms in one set do not have a corresponding waveform in
    the other set.}
  \label{fig:meanwaveforms}
\end{figure}

Mean waveforms for the manually sorted waveforms from a 20 minute
segment of recorded data are shown in Fig.~\ref{fig:meanwaveforms}
alongside a sparse coding basis learned from the corresponding full
recording session. The mean waveforms and basis elements have similar
temporal and spatial extents with a characteristic tri-phasic
structure. They are distributed across the electrode in a similar
manner with a higher density of waveforms in the superficial layers
and more localized waveforms in the deep layers. To assess the meaning
of the sparse coding coefficients in a similar framework to manual
sorting, coefficients for the spike-like bases elements were
thresholded at 50\% of their peak value and waveforms were extracted
from the raw data. Sample waveforms for a subset of the basis
functions are shown in Fig.~\ref{fig:samplewaveforms}, demonstrating
that this simple strategy could be used as an effective pre-processing
step for manual spike sorting. An example is shown in
Fig.~\ref{fig:neuroscope}, where waveforms were extracted based on
thresholded sparse coefficients and loaded into a freely available
tool for processing electrophysiology data~\cite{Hazan:2006fk}, where
standard methods for post-processing can be employed.

\begin{figure}[htp!]
  \centering
  \includegraphics[width=\textwidth,page=10,trim=0 1.25in 4in 0]{Ch2_fig/key2.pdf}
  \caption{\textbf{Waveforms extracted using thresholded spike
      coefficients.} Spike coefficients from 30~s of data was
    thresholded at 50\% of maximum for each coefficient. The waveform
    for each spike coefficient time was extracted from the raw data
    and the contribution from all other basis functions was
    subtracted. A representative sample of several sets of waveforms,
    each corresponding to a basis function, is plotted. Maximum number
    of waveforms shown per plot is 200, with some basis functions
    having fewer than that appearing lighter in color.}
  \label{fig:samplewaveforms}
\end{figure}

\begin{figure}[ht!]
  \centering
  \includegraphics[width=\textwidth,page=12,trim=0 2.5in 1.3in 0]{Ch2_fig/key2.pdf}
  \caption{\textbf{Sparse coding as a preprocessing step for manual
      clustering.} Klusters~\cite{Hazan:2006fk} is part of a suite of
    free tools for processing neurophysiological data. Here, extracted
    waveforms identified from spike coefficients were loaded into
    Klusters for manual modification such as merging of clusters and
    removing multi-unit clusters. Displayed are 100 waveforms in
    columns for 31 spike basis functions.}
  \label{fig:neuroscope}
\end{figure}
\clearpage

To assess the correspondence between manually sorted spike times with
sparse coefficients, cross-correlograms were computed for all pairs of
waveforms and basis functions. Spike and coefficient times were
pre-binned to the frame rate as this is the unit of time used in much
of the analysis in Chap.~\ref{chap:responses} and
\ref{chap:glm}. Pairs with normalized cross-correlogram peaks above a
threshold with overlapping waveforms were identified. Four such pairs
are shown in Fig.~\ref{fig:comparespikes}. Waveforms are similar in
each example case with spike times and coefficient times agreeing to a
high degree. However, many pairs do not show such a close
correspondence with the most common confound being that several basis
function coefficients were used to give the spike times of one
manually sorted neuron. As the ground truth is not know in this case,
it is difficult to ascertain which method is better, and this
direction will not be pursued further. Instead a framework is
developed for generating biophysically plausible model spike data
where any methodology can be properly evaluated with known ground
truth~\cite{einevoll:2011sp} (Sec.~\ref{sec:modelspike}).

\begin{figure}[htp!]
  \centering
  \includegraphics[width=\textwidth,page=11,trim=0 3in 1.8in 0]{Ch2_fig/key2.pdf}
  \caption{\textbf{Comparison of spike times and sparse coefficients.}
    To assess the correspondence between manual spike sorting and
    sparse coefficients, cross-correlograms were computed between
    spike times and coefficients between all pairs of manually sorted
    cells and sparse basis functions. The analysis was done at a
    temporal resolution of the frame rate of the stimuli (150~Hz) as
    this binning is used for all analysis in
    Chap.~\ref{chap:responses} and \ref{chap:glm}. \textbf{(a-d)}
    Shown are four sample candidates where normalized
    cross-correlations were above a heuristic threshold. Shown in each
    plot, starting clockwise at left, are average waveforms, basis
    functions, normalized cross-correlograms, and plot of binned spike
    times in blue and binned coefficient values in red (2nd
    y-axis). Time units are in the frame rate (6.7~ms). Though the
    correspondence is good for many pairs, a number of confounds exist
    that need to be addressed.}
  \label{fig:comparespikes}
\end{figure}
\afterpage{\clearpage}

\subsubsection{Applications using subsets of bases}

Having learned a sparse coding basis and coefficients for an entire
recording dataset, one useful application afforded by this new
representation is to remove components of the data that represent
noise artifacts or uninteresting structure. Fig.~\ref{fig:denoising}
shows a reconstruction of high-pass data where only the spike-like
basis functions are retained and the remaining basis functions are
discarded. The effect is to reduce the noise level in each
channel. This illustrates one reason it is highly desirable to learn
as much of the structure of data as possible, including noise
artifacts, as it is then possible to remove them from the signal. The
computational complexity of the reconstruction is trivial and does not
require further optimizations. Another example of using a subset of
components is given for the LFP in Fig.~\ref{fig:denoisinglfp}. One
possible approach to understanding the structure of the LFP is to
consider reconstructions with only certain classes of basis
functions. In this example, only the basis functions localized in time
and having fine structure separated by lamina were used. Subsequent
analysis can then be applied to this reconstructed data which has
drastically reduced complexity.

\begin{figure}[htp!]
  \centering
  \includegraphics[width=\textwidth,page=13,trim=0 0.5in 3in 0]{Ch2_fig/key2.pdf}
  \caption{\textbf{Denoising spike data.} \textbf{(a,b)} Data in
    Fig.~\ref{fig:spikerecon} reconstructed using a subset of basis
    functions (Fig.~\ref{fig:meanwaveforms}b) resembling spike
    waveforms. Such denoising is trivial to implement as it does not
    require an additional optimization. \textbf{(c)} Sparse
    coefficients for the subset of basis functions.}
  \label{fig:denoising}
\end{figure}

% Another example application is removal of 60~Hz line noise in the
% LFP. A subset of basis functions learned from the LFP
% (Fig.~\ref{fig:lfpbasis}) show structure coherent across channels at
% 60~Hz and 120~Hz. The degree of line noise is not stationary during
% the recording and notch filtering in the time or frequency domain can
% introduce undesirable artifacts. Fig.~\ref{fig:denoisinglfp} shows the
% average power spectrum of the raw data, reconstruction with all basis
% functions, and reconstruction excluding the basis functions containing
% line noise. 

\begin{figure}[htp!]
  \centering
  \includegraphics[width=\textwidth,page=14,trim=0 0.5in 3in 0]{Ch2_fig/key2.pdf}
  \caption{\textbf{Reduced representation of LFP data.}
    \textbf{(a,b)} Data in Fig.~\ref{fig:lfprecon} reconstructed using
    a subset of basis functions with localized kernels and fine
    laminar structure (Fig.~\ref{fig:lfpbasis}). Such a reduced
    representation of complex data can be used for further analyses or
    applications such as event detection. In this example, a large
    event in the superficial lamina with a specific structure is
    readily apparent in the reduced representation but is lost in the
    complexity of the original data. \textbf{(c)} Sparse coefficients
    for the reduced representation.}
  \label{fig:denoisinglfp}
\end{figure}


% \subsubsection{Compression}

% \begin{figure}[ht]
%   \centering
%   \includegraphics[width=\textwidth,page=15,trim=0 8.5in 3in 0]{Ch2_fig/key2.pdf}
%   \caption{\textbf{Compression.} Sample compression rates using
%     generic gzip level 4 HDF5 storage for \textbf{(a)} high-pass data
%     using 0.5\% sparsity \textbf{(b)} low-pass data with 8\%
%     sparsity. All metadata for reconstructing the data is contained in
%     the HDF5 files as well.}
%   \label{fig:compression}
% \end{figure}

\subsection{Sparse coding on model spike and LFP data}
\label{sec:modelspike}

Polytrode data is statistically rich and convolutional sparse coding
is an effective means for decomposing its complex structure into a
more tractable representation. In order to evaluate sparse coding,
particularly in an applied setting such as spike sorting, it is
instructive to work with a biophysically faithful model where the
mechanism for generating the data is completely known. Presented here
is model spike data generated from compartmental models of neurons
with know morphology using the LFPy framework~\cite{linden2011}. Given
a physical description of a recording device and electric currents in
all compartments of the model neurons, one can calculate the potential
at a given time at each recording site using a line source
approximation~\cite{Holt:fk}. Twenty neurons were distributed randomly
about a model 32-channel polytrode. Currents were injected with random
arrival times into a set of 3 channels distributed along the dendritic
trees. In this illustrative example, only a single neuron model was
used and no noise was added. Results are displayed in
Fig.~\ref{fig:lfpylearning}. Spike waveforms were spatially localized
to a greater degree than in real data, which made learning more
difficult as knowledge of the geometry of the channels could not be
exploited. The model data is useful for characterizing and accounting
for several confounds in the representation if the intended
application is spike sorting. These include waveforms being
represented by more than one basis function
(Fig.~\ref{fig:lfpylearning}a-b) and waveforms with similar shapes but
different amplitudes being represented with the same set of basis
functions~(Fig.~\ref{fig:lfpylearning}c). Sparse coding parameters in
this example could be tuned to eliminate these confounds, but with
real data, such tuning can be difficult to perform in a systematic
way. One strategy is to formulate more sophisticated sparse coding
models to specifically address these shortcomings. Another approach is
to account for confounds in processing the coefficients
themselves. Model data provides a framework for evaluating both
strategies.

\begin{figure}[htp!]
  \centering
  \includegraphics[width=\textwidth,page=16,trim=0 0 0 0]{Ch2_fig/key2.pdf}
  \caption{\textbf{Biophysically faithful model spike data.} To
    demonstrate several confounds to interpreting spike sparse
    coefficients as spike times when waveforms are localized to one or
    a few channels, model spike data was generated from 20 randomly
    placed pyramidal neurons, high-pass filtered as is done with
    real data, and learned a 30 component basis. Model data,
    reconstruction, learned basis, as well as coefficients with no
    threshold and coefficients thresholded at 50\% of their maxima are
    displayed. Though the learning is robust to added gaussian noise,
    no noise was added in this example to aid visualization. The inset
    plots demonstrate 2 common confounds. Colors in the learned basis
    correspond to coefficient ellipsoid colors. \textbf{(a,b)} Several
    basis functions are learned and used to represent one spike
    waveform. Through a simple thresholding heuristic this confound is
    removed for \textbf{(a)}, but not for \textbf{(b)}. \textbf{(c)}
    The same set of basis functions are used to represent two spike
    waveforms of similar shape but differing amplitude. This can be
    addressed in a tool such as Klusters~\cite{Hazan:2006fk} (see
    Fig.~\ref{fig:neuroscope}).}
  \label{fig:lfpylearning}
\end{figure}

Generating biophysically faithful model LFP data is considerably more
difficult as the LFP represents the coordinated activity of a large
population of neurons. Here, a simplistic model was used to examine
the effects of line noise on the learning of LFP bases. The statistics
of line noise is quite different from physiological data and is not
well represented by the sparse coding model used here. To better
understand how a high level of line noise impacts learning, model data
was generated using a ground-truth basis of a set of spatio-temporal
gabor functions with coefficients sampled from a Bernoulli
distribution. Fig.~\ref{fig:modellfp}a shows the ground truth
basis. To this generated signal, increasing levels of a rectified
sinusoidal line noise was added. Fig.~\ref{fig:modellfp}b shows the
results of learning a basis where the line noise is similar in
relative magnitude to the line noise in real polytrode data. The basis
functions are faithfully recovered. However, if the noise is increased
to a degree where it dominates the signal, as with
Fig.~\ref{fig:modellfp}c, the basis functions begin to entrain some of
the structure of the noise. However, they are still able to faithfully
recover to a remarkable degree the structure of the ground truth
basis.

\begin{figure}[htp!]
  \centering
  \includegraphics[width=\textwidth,page=17,trim=0 0 2.5in 0]{Ch2_fig/key2.pdf}
  \caption{\textbf{Model LFP data with line noise and recovering
      ground-truth basis.}  \textbf{(a)} A model basis consisting of a
    set of spatiotemporal gabors. \textbf{(b)} A basis learned from
    model data generated from the ground truth basis with Bernoulli
    sampled coefficients. A rectified sinusoid was added as model line
    noise. \textbf{(c)} Four model data samples, reconstructions, and
    inferred coefficients for test cases corresponding to the basis in
    \textbf{(b)}. \textbf{(d)} A basis learned from model data which
    is dominated by line noise as well as gaussian noise. Some basis
    elements are distorted but faithfully recover much of the basic
    structure in the ground truth basis. \textbf{(e)} Model samples,
    reconstructions, and coefficients for the case with high
    noise. Coefficients are plotted as temperature maps with zero
    values in blue.}
  \label{fig:modellfp}
\end{figure}
\clearpage

\section{Discussion}

A novel application of a convolutional sparse coding algorithm to
polytrode data recorded from visual cortex of anesthetized cats was
presented. When applied to high-pass filtered polytrode data, the
algorithm learns a set of spike waveform components along the length
of the polytrode. These components and coefficients have close
correspondence to waveforms and spike times extracted using a
conventional spike sorting algorithm. When applied to the LFP, the
learned representation consists of basis elements that have structure
separated across lamina at different temporal scales. Together these
methods provide a powerful set of new tools for exploratory analysis
of polytrode data. In Chap.~\ref{chap:responses}, this method is
applied to all recorded data from several polytrode experiments and
its utility is demonstrated in characterizing the laminar distribution
of responses to a wide range of stimuli.

Sparse coding as applied to polytrode data provides several  immediate
applications. Given even the conservative settings of learning and
inference of spike basis functions in this chapter, a 100-fold
compression can be achieved. Sparse coefficients can be stored with
basis functions as metadata in lieu of actual data, reducing hardware
requirements and bandwidth needed to transmit data. Compression will
become increasingly necessary as the number of polytrode recording
contacts grow into the hundreds and thousands in the future. It may
even need to be built into the head-stage and acquisition system to be
run online. Sparse coding provides a fast way to denoise data for
subsequent processing. Denoising involves reconstructing data while
omitting a subset of basis functions, which does not require
optimizations. Learning basis functions on different portions of the
data can be used to identify non-stationarities, such as electrode
drift, that can be addressed by performing learning and inference
adaptively over a dataset.

The most promising application of sparse coding is either as a
pre-processing step for manual spike sorting or as a stand-alone
algorithm. Spike sorting is a highly underdetermined, difficult
computational problem while also being a necessary processing step for
all subsequent analysis of electrophysiology data. Existing methods,
such as clustering algorithms based on mixtures of gaussians, extract
events from data using heuristic thresholds and use feature spaces
that are highly tailored and artificial. These methods have numerous
deficiencies such as not being able to model overlapping waveforms,
not modeling all the data but just extracted waveforms, and scaling
poorly with increased dimensionality of the signal, all areas where
sparse coding excels. However, before the methods in this chapter can
be incorporated into a full spike sorter, it is essential to handle
certain confounds discussed in Sec.~\ref{sec:modelspike}, such as
single waveforms being represented by multiple basis functions and a
single basis function being used with different magnitudes to explain
different waveforms of different neurons. Sparse coefficients are
analog whereas action potentials are all-or-none events, posing a
difficult issue of the interpretability of the magnitude of sparse
coefficients, requiring subsequent thresholding that may be difficult
to perform in a principled manner. The coefficients could be
constrained to be binary, but such inference problems are typically
intractable, though convex relaxations may exist. An additional
difficulty in interpreting coefficients is that sparse representations
can be brittle. If data is perturbed in a trivial way, coefficients
may change substantially. Generating biophysically realistic model
data incorporating as much of the complexity of the real data is a
promising strategy for evaluating spike sorting performance and will
be the subject of future study.

% Statistical models applied to data can test computational
% models as well. Statistical models can explicitly incorporate
% biophysical modeling information, such as morphology and non-linear
% channel dynamics~\cite{Huys:2006ys,Huys:2009vn}.

% Though sparse coding seems well suited for representing spiking
% activity, it is not clear if it is as well suited for representing the
% LFP, as there are no clear sparse causes for LFP. Oscillations in
% certain frequency bands are a distinctive characteristic of
% LFP. Oscillations could be better represented using an
% autoregressive formulation of the objective, though little has been
% attempted in this area

% 1. Line noise not well represented (showed model data)
% not causal.